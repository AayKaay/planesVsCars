{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PlaneVsCarsCNN.ipynb",
      "provenance": [],
      "mount_file_id": "1YIz7qjQNoYQW3hl0yz6mlCBBdLCu4lNC",
      "authorship_tag": "ABX9TyP1BqiKe09xm0tmwGZaedHa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AayKaay/planesVsCars/blob/master/PlaneVsCarsCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMuqPieJUTWt",
        "colab_type": "text"
      },
      "source": [
        "# **Binary classification using Convulational Neural Networks**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Model was trained using total 400 images; planes and cars, 200 each.\n",
        "Testing dataset had 50 pictures.\n",
        "Each batch had 25 images so total batch was 16.\n",
        "Approx. 94 percent accuracy was achieved.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZhrgnc5gGnp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator \n",
        "from tensorflow.keras.models import Sequential \n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D \n",
        "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense \n",
        "\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "\n",
        "\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
        "%load_ext tensorboard\n",
        "!rm -rf ./logs/ "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbMWi3c-mv2P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.applications.inception_v3 as inception\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDX9PN5Hpfxh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "69df0a34-03bb-4668-d223-4b84118f11d1"
      },
      "source": [
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "print(log_dir)\n",
        "datetime.datetime.now()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "logs/fit/2020-04-29_20-38-37\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "datetime.datetime(2020, 4, 29, 20, 38, 37, 514904)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JPzRkWPj79X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "outputId": "7c732ad2-32bf-40db-c310-35db785bda39"
      },
      "source": [
        "\n",
        "train_data_dir = '/content/drive/My Drive/v_data/train'\n",
        "validation_data_dir = '/content/drive/My Drive/v_data/test'\n",
        "nb_train_samples =400 \n",
        "nb_validation_samples = 100\n",
        "epochs = 10\n",
        "batch_size = 16\n",
        "img_width = 224\n",
        "img_height = img_width\n",
        "if K.image_data_format() == 'channels_first': \n",
        "    input_shape = (3, img_width, img_height) \n",
        "else: \n",
        "    input_shape = (img_width, img_height, 3) \n",
        "\n",
        "model = Sequential() \n",
        "dd = inception.InceptionV3(include_top=False,weights='imagenet',pooling='avg')\n",
        "# dd = ResNet50(include_top=False,weights='imagenet',pooling='avg')\n",
        "model.add(dd)\n",
        "# model.add(Conv2D(32, (2, 2), input_shape=input_shape)) \n",
        "# model.add(Activation('relu')) \n",
        "# model.add(MaxPooling2D(pool_size=(2, 2))) \n",
        "  \n",
        "# model.add(Conv2D(32, (2, 2))) \n",
        "# model.add(Activation('relu')) \n",
        "# model.add(MaxPooling2D(pool_size=(2, 2))) \n",
        "  \n",
        "# model.add(Conv2D(64, (2, 2))) \n",
        "# model.add(Activation('relu')) \n",
        "# model.add(MaxPooling2D(pool_size=(2, 2))) \n",
        "  \n",
        "# model.add(Flatten()) \n",
        "# model.add(Dense(64)) \n",
        "# model.add(Activation('relu')) \n",
        "model.add(Dropout(0.5)) \n",
        "model.add(Dense(1,activation='sigmoid')) \n",
        "for layer in model.layers[:-2]:\n",
        "    layer.trainable = False\n",
        "# model.add(Activation('sigmoid')) \n",
        "#model.layers[-1].trainable=False\n",
        "# model.layers[len(model.layers)-1].trainab\n",
        "print(model.summary())\n",
        "\n",
        "model.compile(loss='binary_crossentropy', \n",
        "              optimizer='sgd', \n",
        "              metrics=['accuracy']) \n",
        "\n",
        "train_datagen = ImageDataGenerator( \n",
        "    rescale=1. / 255, \n",
        "    shear_range=0.2,\n",
        "    preprocessing_function=preprocess_input, \n",
        "    zoom_range=0.2, \n",
        "    horizontal_flip=True) \n",
        "  \n",
        "test_datagen = ImageDataGenerator(rescale=1. / 255) \n",
        "  \n",
        "train_generator = train_datagen.flow_from_directory( \n",
        "    train_data_dir, \n",
        "    target_size=(img_width, img_height), \n",
        "    batch_size=batch_size, \n",
        "    class_mode='binary') \n",
        "  \n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory( \n",
        "    validation_data_dir, \n",
        "    target_size=(img_width, img_height), \n",
        "    batch_size=batch_size, \n",
        "    class_mode='binary') \n",
        "\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1,\n",
        "                                                      write_images=True)\n",
        "writer = tf.summary.create_file_writer(log_dir)\n",
        "\n",
        "model.fit( \n",
        "    train_generator, \n",
        "    steps_per_epoch=nb_train_samples // batch_size, \n",
        "    epochs=epochs, \n",
        "    validation_data=validation_generator, \n",
        "    validation_steps=nb_validation_samples // batch_size,\n",
        "    callbacks=[tensorboard_callback]) \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "inception_v3 (Model)         (None, 2048)              21802784  \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 21,804,833\n",
            "Trainable params: 2,049\n",
            "Non-trainable params: 21,802,784\n",
            "_________________________________________________________________\n",
            "None\n",
            "Found 400 images belonging to 2 classes.\n",
            "Found 100 images belonging to 2 classes.\n",
            "Epoch 1/10\n",
            "25/25 [==============================] - 10s 385ms/step - loss: 0.2605 - accuracy: 0.8950 - val_loss: 0.0576 - val_accuracy: 0.9896\n",
            "Epoch 2/10\n",
            "25/25 [==============================] - 8s 333ms/step - loss: 0.0682 - accuracy: 0.9850 - val_loss: 0.0354 - val_accuracy: 1.0000\n",
            "Epoch 3/10\n",
            "25/25 [==============================] - 8s 324ms/step - loss: 0.0466 - accuracy: 0.9900 - val_loss: 0.0234 - val_accuracy: 1.0000\n",
            "Epoch 4/10\n",
            "25/25 [==============================] - 8s 333ms/step - loss: 0.0433 - accuracy: 0.9875 - val_loss: 0.0159 - val_accuracy: 1.0000\n",
            "Epoch 5/10\n",
            "25/25 [==============================] - 8s 328ms/step - loss: 0.0236 - accuracy: 0.9950 - val_loss: 0.0173 - val_accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "25/25 [==============================] - 8s 327ms/step - loss: 0.0166 - accuracy: 1.0000 - val_loss: 0.0098 - val_accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "25/25 [==============================] - 9s 341ms/step - loss: 0.0201 - accuracy: 0.9975 - val_loss: 0.0136 - val_accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "25/25 [==============================] - 8s 340ms/step - loss: 0.0217 - accuracy: 0.9900 - val_loss: 0.0139 - val_accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "25/25 [==============================] - 8s 325ms/step - loss: 0.0162 - accuracy: 0.9950 - val_loss: 0.0131 - val_accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "25/25 [==============================] - 8s 329ms/step - loss: 0.0206 - accuracy: 0.9950 - val_loss: 0.0126 - val_accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fde91d97cf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoXc8CTHsoIL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f455ab5a-e62f-47fa-989c-cea7bdb866fa"
      },
      "source": [
        "%tensorboard --logdir logs/fit"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UsageError: Line magic function `%tensorboard` not found.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-MNGeFc6JdF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d0b65b17-7d2e-4d69-c625-af962870f18d"
      },
      "source": [
        "import cv2 \n",
        "import numpy as np\n",
        "imgtest = []\n",
        "img = cv2.imread('/content/19.jpg')\n",
        "img1 = cv2.imread('/content/Transpo_G70_TA-518126.jpg')\n",
        "img2 = cv2.imread('/content/14.jpg')\n",
        "imgtest.append(img)\n",
        "imgtest.append(img1)\n",
        "imgtest.append(img2)\n",
        "imgg=[]\n",
        "imgt = cv2.resize(imgtest[0],(224,224))\n",
        "imgt = np.reshape(imgt,[1,224,224,3])\n",
        "imgg.append(imgt)\n",
        "imgt = cv2.resize(imgtest[1],(224,224))\n",
        "imgt = np.reshape(imgt,[1,224,224,3])\n",
        "imgg.append(imgt)\n",
        "imgt = cv2.resize(imgtest[2],(224,224))\n",
        "imgt = np.reshape(imgt,[1,224,224,3])\n",
        "imgg.append(imgt)\n",
        "\n",
        "y_pred=[]\n",
        "from google.colab.patches import cv2_imshow\n",
        "for x in imgg:\n",
        "    y_pred.append(model.predict(x))\n",
        "print((y_pred))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([[8.759889e-26]], dtype=float32), array([[1.6421174e-16]], dtype=float32), array([[3.631916e-23]], dtype=float32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XDWOlVIwW4UI",
        "colab": {}
      },
      "source": [
        "import cv2 \n",
        "import numpy as np\n",
        "imgtest = []\n",
        "img = cv2.imread('/content/19.jpg')\n",
        "img1 = cv2.imread('/content/Transpo_G70_TA-518126.jpg')\n",
        "img2 = cv2.imread('/content/14.jpg')\n",
        "imgtest.append(img)\n",
        "imgtest.append(img1)\n",
        "imgtest.append(img2)\n",
        "imgg=[]\n",
        "imgt = cv2.resize(imgtest[0],(224,224))\n",
        "imgt = np.reshape(imgt,[1,224,224,3])\n",
        "imgg.append(imgt)\n",
        "imgt = cv2.resize(imgtest[1],(224,224))\n",
        "imgt = np.reshape(imgt,[1,224,224,3])\n",
        "imgg.append(imgt)\n",
        "imgt = cv2.resize(imgtest[2],(224,224))\n",
        "imgt = np.reshape(imgt,[1,224,224,3])\n",
        "imgg.append(imgt)\n",
        "\n",
        "y_pred=[]\n",
        "for x in imgg:\n",
        "    cv2.imshow(x)\n",
        "    y_pred.append(model.predict_classes(x))\n",
        "print((y_pred))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}